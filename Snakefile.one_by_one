GMARK_QUERIES = [
    'query_1', 
    'query_2', 
    'query_3', 
    'query_4', 
    'query_5', 
    'query_6', 
    'query_7', 
    'query_8', 
    'query_9', 
    'query_10', 
    'query_11', 
    'query_12', 
    'query_13', 
    'query_14', 
    'query_15', 
    'query_16', 
    'query_17', 
    'query_18', 
    'query_19', 
    'query_20', 
    'query_21', 
    'query_22', 
    'query_23', 
    'query_24', 
    'query_25', 
    'query_26',
    'query_27', 
    'query_28', 
    'query_29', 
    'query_30'
]

WIKIDATA_QUERIES = [
    'query_1', 
    'query_2', 
    'query_3', 
    'query_4', 
    'query_5', 
    'query_6', 
    'query_7', 
    'query_8', 
    'query_9', 
    'query_10', 
    'query_11', 
    'query_12', 
    'query_13', 
    'query_14', 
    'query_15'
]

# GMark

rule gmark_run_all:
    input:
        expand('queries/gmark/{query}.sparql', query=GMARK_QUERIES)

rule gmark_run_sage_ptc:
    input:
        ancient('queries/gmark/{query}.sparql')
    output:
        'output/gmark/sage_ptc/{query}.csv'
    shell:
        'node ./scripts/sage-ptc.js http://localhost:8080/sparql http://example.org/datasets/hdt/shop10M {output} -f {input} --timeout 1800'

rule gmark_merge_sage_ptc:
    input:
        expand('output/gmark/sage_ptc/{query}.csv', query=GMARK_QUERIES)
    output:
        'output/gmark/sage_ptc.csv'
    shell:
        'bash ./scripts/merge_csv.sh {input} > {output}'

rule gmark_run_sage_client_ptc:
    output:
        'output/gmark/sage_client_ptc.csv'
    shell:
        'node ./scripts/sage-client-ptc.js http://localhost:8080/sparql http://example.org/datasets/hdt/shop10M {output} --workload GMark --timeout 1800'

rule gmark_run_sage_client_multi:
    input:
        ancient('queries/gmark/{query}.sparql')
    output:
        'output/gmark/sage_client_multi/{query}.csv'
    shell:
        'node ./scripts/query-sage.js http://localhost:8080/sparql http://example.org/datasets/hdt/shop10M {output} -f {input} --method multi --timeout 1800'

rule gmark_merge_sage_client_multi:
    input:
        expand('output/gmark/sage_client_multi/{query}.csv', query=GMARK_QUERIES)
    output:
        'output/gmark/sage_client_multi.csv'
    shell:
        'bash ./scripts/merge_csv.sh {input} > {output}'

rule gmark_run_virtuoso:
    input:
        ancient('queries/gmark/{query}.sparql')
    output:
        'output/gmark/virtuoso/{query}.csv'
    shell:
        'python ./scripts/query-endpoint.py virtuoso http://localhost:8890/sparql http://example.org/datasets/shop10M {output} -f {input} --timeout 1800'

rule gmark_merge_virtuoso:
    input:
        expand('output/gmark/virtuoso/{query}.csv', query=GMARK_QUERIES)
    output:
        'output/gmark/virtuoso.csv'
    shell:
        'bash ./scripts/merge_csv.sh {input} > {output}'

rule gmark_run_fuseki:
    input:
        ancient('queries/gmark/{query}.sparql')
    output:
        'output/gmark/fuseki/{query}.csv'
    shell:
        'python ./scripts/query-endpoint.py fuseki http://localhost:8890/sparql http://example.org/datasets/shop10M {output} -f {input} --timeout 1800'

rule gmark_merge_fuseki:
    input:
        expand('output/gmark/fuseki/{query}.csv', query=GMARK_QUERIES)
    output:
        'output/gmark/fuseki.csv'
    shell:
        'bash ./scripts/merge_csv.sh {input} > {output}'

rule gmark_quantum_impact:
    input:
        ancient('output/gmark/sage_ptc_60sec.csv'),
        ancient('output/gmark/sage_ptc_10sec.csv'),
        ancient('output/gmark/sage_ptc_1sec.csv')
    output:
        'output/gmark/quantum_impact.csv'
    shell:
        'bash ./scripts/merge_csv.sh {input} > {output}'

rule gmark_tpc_client_k_impact:
    input:
        ancient('output/gmark/sage_client_ptc_2.csv'),
        ancient('output/gmark/sage_client_ptc_5.csv'),
        ancient('output/gmark/sage_client_ptc_10.csv')
    output:
        'output/gmark/quantum_impact.csv'
    shell:
        'bash ./scripts/merge_csv.sh {input} > {output}'

rule gmark_approaches_comparison:
    input:
        ancient('output/gmark/sage_ptc_60sec.csv'),
        ancient('output/gmark/sage_client_ptc_5.csv'),
        ancient('output/gmark/fuseki.csv'),
        ancient('output/gmark/virtuoso.csv')
    output:
        'output/gmark/approaches_comparison.csv'
    shell:
        'bash ./scripts/merge_csv.sh {input} > {output}'

# Wikidata

rule wikidata_run_all:
    input:
        expand('queries/wikidata/{query}.sparql', query=WIKIDATA_QUERIES)

rule wikidata_run_sage_ptc:
    input:
        ancient('queries/wikidata/{query}.sparql')
    output:
        'output/wikidata/sage_ptc/{query}.csv'
    shell:
        'node ./scripts/sage-ptc.js http://localhost:8080/sparql http://example.org/datasets/hdt/wikidata {output} -f {input} --timeout 1800'

rule wikidata_merge_sage_ptc:
    input:
        expand('output/wikidata/sage_ptc/{query}.csv', query=WIKIDATA_QUERIES)
    output:
        'output/wikidata/sage_ptc.csv'
    shell:
        'bash ./scripts/merge_csv.sh {input} > {output}'

rule wikidata_run_sage_client_ptc:
    output:
        'output/wikidata/sage_client_ptc.csv'
    shell:
        'node ./scripts/sage-client-ptc.js http://localhost:8080/sparql http://example.org/datasets/hdt/wikidata {output} --workload Wikidata --timeout 1800'

rule wikidata_run_sage_client_multi:
    input:
        ancient('queries/wikidata/{query}.sparql')
    output:
        'output/wikidata/sage_client_multi/{query}.csv'
    shell:
        'node ./scripts/query-sage.js http://localhost:8080/sparql http://example.org/datasets/hdt/wikidata {output} -f {input} --method multi --timeout 1800'

rule wikidata_merge_sage_client_multi:
    input:
        expand('output/wikidata/sage_client_multi/{query}.csv', query=WIKIDATA_QUERIES)
    output:
        'output/wikidata/sage_client_multi.csv'
    shell:
        'bash ./scripts/merge_csv.sh {input} > {output}'

rule wikidata_run_fuseki:
    input:
        ancient('queries/wikidata/{query}.sparql')
    output:
        'output/wikidata/fuseki/{query}.csv'
    shell:
        'python ./scripts/query-endpoint.py fuseki http://localhost:8890/sparql http://example.org/datasets/wikidata {output} -f {input} --timeout 1800'

rule wikidata_merge_fuseki:
    input:
        expand('output/wikidata/fuseki/{query}.csv', query=WIKIDATA_QUERIES)
    output:
        'output/wikidata/fuseki.csv'
    shell:
        'bash ./scripts/merge_csv.sh {input} > {output}'

# rule wikidata_quantum_impact:
#     input:
#         ancient('output/wikidata/sage_ptc_60sec.csv'),
#         ancient('output/wikidata/sage_ptc_10sec.csv'),
#         ancient('output/wikidata/sage_ptc_1sec.csv')
#     output:
#         'output/wikidata/quantum_impact.csv'
#     shell:
#         'bash ./scripts/merge_csv.sh {input} > {output}'

# rule wikidata_tpc_client_k_impact:
#     input:
#         ancient('output/wikidata/sage_client_ptc_2.csv'),
#         ancient('output/wikidata/sage_client_ptc_5.csv'),
#         ancient('output/wikidata/sage_client_ptc_10.csv')
#     output:
#         'output/wikidata/quantum_impact.csv'
#     shell:
#         'bash ./scripts/merge_csv.sh {input} > {output}'

rule wikidata_approaches_comparison:
    input:
        ancient('output/wikidata/sage_ptc_60sec.csv'),
        ancient('output/wikidata/sage_client_ptc_5.csv'),
        ancient('output/wikidata/fuseki.csv')
    output:
        'output/wikidata/approaches_comparison.csv'
    shell:
        'bash ./scripts/merge_csv.sh {input} > {output}'